### dummy data for development
dat = read.csv("Level6_Genus.csv", header=T,row.name=1)
setwd("~/GitHub/micRoclean_development")
### dummy data for development
dat = read.csv("Level6_Genus.csv", header=T,row.name=1)
batch = dat$Batch
group = dat$Groups
index = grep(".*.g__*", colnames(dat)) # keep if have IDed datus
dat = dat[,index]
comp = data.frame(colnames(dat))
comp$compare = sub(".*.g__", "", colnames(dat)) # subset to only datus
index2 = which(comp$compare=="")
comp = comp[-index2,]
index = grep(".*.g__*", colnames(dat)) # keep if have IDed datus
dat = dat[,index]
colnames(dat) = sub(".*.g__", "", colnames(dat)) # subset to only datus
index2 = which(names(dat)=="")
dat = dat[,-index2] # remove if no value
## dummy res data for pipeline2
### dummy meta data
control = group
control = control == "Negative Control"
sample = group
sample[!sample == "Control"] = "Plasma"
dat = as.matrix(dat)
meta = data.frame("is_control" = control,
"sample" = sample,
"batch" = batch)
rownames(meta) = rownames(dat)
### dummy technical replicates (p2s3)
technical_replicates = data.frame("Batch1" = c("Old_trimmed_2", "Old_trimmed_86",
"Old_trimmed_85", "Old_trimmed_49",
"Old_trimmed_38", "Old_trimmed_3",
"Old_trimmed_13", "Old_trimmed_26"),
"Batch2" = c("New_trimmed_29", "New_trimmed_35",
"New_trimmed_41", "New_trimmed_47",
"New_trimmed_53", "New_trimmed_59",
"New_trimmed_65", "New_trimmed_71"))
# wrap dataframe for technical replicates in each batch ordered by match (line ~336 original_pipeline2.R)
count_replicate = list()
PA_replicate = list()
for (i in 1:dplyr::n_distinct(meta$batch)) {
vals = technical_replicates[,i]
count_replicate[[i]] = data.frame(t(counts[vals,]))
# create presence absence matrices
k = count_replicate[[i]]
k = k %>%
mutate(ifelse(k>0, 1, 0))
PA_replicate[[i]] = k
}
counts = dat
for (i in 1:dplyr::n_distinct(meta$batch)) {
vals = technical_replicates[,i]
count_replicate[[i]] = data.frame(t(counts[vals,]))
# create presence absence matrices
k = count_replicate[[i]]
k = k %>%
mutate(ifelse(k>0, 1, 0))
PA_replicate[[i]] = k
}
library(phyloseq) # object wrapper
library(SummarizedExperiment) # object wrapper
library(tidyverse)
library(plotly) # for interactive feature
library(SCRuB) # pipeline 1
library(PERfect) # DFL
install.packages('PERfect')
library(microDecon) # pipeline 2
library(ANCOMBC) # pipeline 2
library(ggVennDiagram) # function 3 - pipeline 2 - comparison across removed
library(shiny) # function 3
library(ANCOMBC) # pipeline 2 step1
library(kappa) # pipeline 2 step3
install.packages('kappa')
install.packages('irr')
# Prune failed taxa from final object
res = data.frame()
rownaems(res) = rownames(counts)
rownames(res) = rownames(counts)
rownames(counts)
rownames(res) = colnames(counts)
# Prune failed taxa from final object
res = matrix('feature' = colnames(counts))
# Prune failed taxa from final object
res = data.frame('feature' = colnames(counts))
ifelse(colnames(counts) %in% s1_res, TRUE, FALSE)
step1 = function(counts, meta) {
phyloseq = wrap_phyloseq(counts, meta)
# run differential analysis
s1 = ancombc(phyloseq = phyloseq, assay_name = "counts",
group = "batch", p_adj_method = "BH",  lib_cut = 0,
formula = "batch",
struc_zero = TRUE, neg_lb = FALSE,
tol = 1e-5, max_iter = 100, conserve = FALSE,
alpha = 0.05, global = TRUE)
# create results matrix
s1_res = do.call(cbind, s1$res)
# identify column for diff results
col = ncol(s1_res)
# return indices for which differentially abundant across batches
ind = which(s1_res[,col]==TRUE)
# return list of tagged contaminant features
return(s1_res[ind,1])
}
s1_res = step1(counts, meta)
wrap_phyloseq = function(counts, meta) {
counts = t(counts) # transpose to fit with expectation of phyloseq object
OTU = otu_table(counts, taxa_are_rows = TRUE)
META = sample_data(meta)
tax_mat = matrix(rownames(counts),nrow=nrow(counts),ncol=1)
rownames(tax_mat) = rownames(counts)
TAX = tax_table(tax_mat)
return(phyloseq(OTU, META, TAX))
}
NP_Order = function(counts){
#arrange counts in order of increasing number of samples taxa are present in
NP = names(sort(apply(counts, 2, Matrix::nnzero)))
return(NP)
}
FL = function(counts, removed){
#X - data matrix
#Ind - only jth taxon removed to calculate FL
p = dim(counts)[2]#total #of taxa
Norm_Ratio = rep(1, p)
# Check the format of X
if(!(class(counts) %in% c("matrix"))){counts = as.matrix(counts)}
#Order columns by importance
Order.vec = NP_Order(counts)
counts = counts[,Order.vec] #properly order columns of X
Order_Ind = seq_len(length(Order.vec))
Netw = t(counts)%*%counts
#Taxa at the top of the list have smallest number of connected nodes
for (i in seq_len(p)){
Ind = Order_Ind[-i]
#define matrix X_{-J}'X_{-J} for individual filtering loss
Netw_R = Netw[Ind, Ind]
Norm_Ratio[i] =  sum(Netw_R*Netw_R)
}
FL = 1 - Norm_Ratio/sum(Netw*Netw)
names(FL) =  colnames(counts)
FL_value = sum(FL[names(FL) %in% removed])
return(FL_value)
# return(FL = data.frame(FL))
}
s1_res = step1(counts, meta)
ifelse(colnames(counts) %in% s1_res, TRUE, FALSE)
## dummy res data for pipeline2
res = read.csv('res_toydata.csv')
View(res)
samp = c(TRUE, FALSE, TRUE, TRUE)
sum(samp)
# return column with summed cases where feature was true
res$remove = rowSums(res)
